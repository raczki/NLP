{"cells":[{"cell_type":"markdown","metadata":{"id":"kBLpTr7plguX"},"source":["<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Procesamiento de lenguaje natural\n","## Sentiment analysis con Embeddings + LSTM"]},{"cell_type":"markdown","metadata":{"id":"9W6nuajhlqZD"},"source":["### Objetivo\n","El objetivo es utilizar las críticas de compradores de ropa para que el sistema determine la evaluación del comprador y su crítica (cuantas estrellas le asigna al producto)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i6zvzv3qZ6xS"},"outputs":[],"source":["!pip install --upgrade --no-cache-dir gdown --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hCpOVzJdl8_p"},"outputs":[],"source":["import numpy as np\n","import random\n","import io\n","import pickle\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","from keras.utils.np_utils import to_categorical\n","from tensorflow.keras.utils import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Embedding"]},{"cell_type":"markdown","metadata":{"id":"8UPeRkrAmbF3"},"source":["### Datos\n","Utilizaremos como dataset críticas de compradores de ropa (eCommerce) los cuales puntuaron a cada prenda con un puntaje de 1 a 5 estrellas.\\\n","Referencia del dataset: [LINK](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews/version/1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C7jLvTU3lSyL"},"outputs":[],"source":["# Descargar la carpeta de dataset\n","import os\n","import gdown\n","if os.access('clothing_ecommerce_reviews.csv', os.F_OK) is False:\n","    url = 'https://drive.google.com/uc?id=1Urn1UFSrodN5BuW6-sc_igtaySGRwhV8'\n","    output = 'clothing_ecommerce_reviews.csv'\n","    gdown.download(url, output, quiet=False)\n","else:\n","    print(\"El dataset ya se encuentra descargado\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o-SV1P3dnD1J"},"outputs":[],"source":["# Armar el dataset\n","df = pd.read_csv('clothing_ecommerce_reviews.csv')\n","df.drop(columns = ['Unnamed: 0'], inplace = True)\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"q-OwSePKm-FK"},"source":["### 1 - Limpieza de datos\n","Alumno:\n","- Del dataset unicamente utilizar las columnas \"Review Text\" y \"Rating.\n","- Tranformar el rating 1-5 a una escala numérica de 0 a 4.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-hc7-AmYnPC3"},"outputs":[],"source":["df_reviews = df.loc[:, ['Review Text', 'Rating']].dropna()\n","df_reviews['Rating'] = df_reviews['Rating'] - 1\n","df_reviews.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sZtvASVOn3ty"},"outputs":[],"source":["# Alumno: Observar como está distribuido el dataset respecto a la columna Rating\n","# es decir, observar que tan balanceado se encuentra respecot a cada clase\n","print(df_reviews.shape)\n","df_reviews.groupby(by=['Rating']).count()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_weigth_balance(df):\n","    '''\n","    Función para graficar el balance de calses con sus porcentajes\n","    '''\n","    # Calcular counts y porcentajes\n","    valid_categories = df\n","    category_counts = valid_categories.value_counts()\n","    total_count = len(valid_categories)\n","    category_percentages = (category_counts / total_count) * 100\n","\n","    # Grafico\n","    plt.figure(figsize=(10, 6))\n","    bars = plt.bar(category_counts.index, category_counts, tick_label = category_counts.index)\n","    plt.xlabel(\"Rating\")\n","    plt.ylabel(\"Review Text\")\n","    plt.bar_label(bars, label_type='center')\n","\n","    # Agregar porcentajes sobre las barras\n","    for i, bar in enumerate(bars):\n","        height = bar.get_height()\n","        category = category_counts.index[i]\n","        if category in category_percentages:\n","            percentage = category_percentages[category]\n","            plt.text(bar.get_x() + bar.get_width() / 2, height, f\"{percentage:.2f}%\", ha=\"center\", va=\"bottom\")\n","            \n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_weigth_balance(df_reviews['Rating'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_reviews['Rating'] = df_reviews['Rating'].replace({1: 0, 2: 0, 3: 1, 4: 2})\n","plot_weigth_balance(df_reviews['Rating'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gVJ_RVi4o1h3"},"outputs":[],"source":["# Alumno: tomar la columna de las review y almacenarlo todo en un vector numpy de reviews\n","text_sequences = df_reviews['Review Text'].values\n","text_sequences.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4nT5Un_co65Q"},"outputs":[],"source":["# Alumno: Cuantas reviews (rows) hay para evaluar?\n","len(text_sequences)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HP5uN9tqpHu_"},"outputs":[],"source":["# Alumno: Concatenar todas las reviews para armar el corpus\n","corpus = ' '.join(text_sequences)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FEzmePgdpf74"},"outputs":[],"source":["# Alumno: ¿Cuál es la longitud de ese corpus?\n","len(corpus)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MYeJLdDmpvOe"},"outputs":[],"source":["# Alumno: Utilizar \"text_to_word_sequence\" para separar las palabras en tokens\n","# recordar que text_to_word_sequence automaticamente quita los signos de puntuacion y pasa el texto a lowercase\n","from keras.preprocessing.text import text_to_word_sequence\n","tokens = text_to_word_sequence(corpus)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M6L-fnWAp_lA"},"outputs":[],"source":["# Alumno: Dar un vistazo a los primeros 20 tokens/palabras\n","tokens[:20]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-8QgwwMUqG0d"},"outputs":[],"source":["# Alumno: ¿Cuántos tokens/palabras hay?\n","len(tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TFukNZdOsZ8_"},"outputs":[],"source":["# Alumno: Tokenizar las palabras con el Tokenizer de Keras\n","# Definir una máxima cantidad de palabras a utilizar:\n","# num_words --> the maximum number of words to keep, based on word frequency.\n","# Only the most common num_words-1 words will be kept.\n","from keras.preprocessing.text import Tokenizer\n","num_words = 2000\n","vocab_size = num_words\n","tok = Tokenizer(num_words=2000) \n","tok.fit_on_texts(tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JnR1tlqZy94X"},"outputs":[],"source":["# Alumno: Obtener el diccionario de palabra (word) a índice\n","# y observar la cantidad total del vocabulario\n","word_index = tok.word_index\n","print(f'tamaño de vocabulario : {len(word_index)}')\n","index_word = tok.index_word\n","# Imprimo las primeras 50 palabras del diccionario\n","for t, t_word in list(index_word.items())[:50]:\n","    print(t, ':', t_word)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AvWzzSretQXf"},"outputs":[],"source":["# Alumno: Convertir las palabras/tokens a números\n","sequences = tok.texts_to_sequences(text_sequences)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"za73M5SRtbrP"},"outputs":[],"source":["# Alumno: Determinar cual es la oración más larga\n","seqs_lenght = [len(s) for s in sequences]\n","print(f'el máximo es {max(seqs_lenght)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oCkO9Wc9tls1"},"outputs":[],"source":["# Alumno: Realizar padding de las sentencias al mismo tamaño\n","# tomando de referencia la máxima sentencia\n","from tensorflow.keras.utils import pad_sequences\n","maxlen = 115\n","\n","# Al realizar padding obtener la variable \"X\" (input)\n","X = pad_sequences(sequences, padding='pre', maxlen=maxlen)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kGHHabVdt_aa"},"outputs":[],"source":["# Alumno: Observar las dimensiones de la variable input\n","X.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"llVM-tzQo9_F"},"outputs":[],"source":["# Alumno tomar la columna rating y alcemacenarla en una variable \"y\" transformada a oneHotEncoding\n","# Su shape debe ser equivalente la cantidad de rows del corpus y a la cantidad\n","# de clases que se deseen predecir (en este ejemplo son 5)\n","from tensorflow.keras.utils import to_categorical\n","\n","y = df_reviews['Rating'].copy()\n","y = to_categorical(y, num_classes=3)\n","print(y.shape)\n","y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rmz9A6n4uK4V"},"outputs":[],"source":["# Alumno: Dividir los datos en train y test\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EcDPlhEouQ9E"},"outputs":[],"source":["# Alumno: determinar la dimensiones de entrada y salida\n","in_shape = X_train.shape[1] # max input sentence len\n","out_shape = 3 # binary classification\n","print(\"in_shape\", in_shape, \", out_shape\", out_shape)"]},{"cell_type":"markdown","metadata":{"id":"NpbQHExL6OTu"},"source":["### 2 - Entrenar el modelo con Embeddings + LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NUkuWBsM6cx3"},"outputs":[],"source":["# Entrenar un modelo con LSTM entrenando sus propios embeddings\n","# o utilizando embeddings pre-entrenados.\n","from keras.layers import LSTM\n","from keras.layers import Embedding\n","from keras.layers import Dropout\n","\n","model = Sequential()\n","# input_dim = vocab_size (max n_words)\n","# input_length = setencias con padding a 115\n","# output_dim = 50 --> crear embeddings de tamaño 50\n","model.add(Embedding(input_dim=vocab_size+1, output_dim=50, input_length=in_shape))\n","model.add(LSTM(units=128, return_sequences=True))\n","model.add(Dropout(0.2))\n","model.add(LSTM(units=128, return_sequences=True))\n","model.add(Dropout(0.2))\n","model.add(LSTM(units=64)) # La última capa LSTM no lleva return_sequences\n","\n","model.add(Dense(units=128, activation='relu'))\n","model.add(Dropout(rate=0.2))\n","model.add(Dense(units=out_shape, activation='sigmoid'))\n","\n","model.compile(optimizer=\"adam\",\n","              loss='binary_crossentropy',\n","              metrics=[tf.keras.metrics.Precision(), \n","                       tf.keras.metrics.Recall()])\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hist = model.fit(X_train, y_train, epochs=50, validation_split=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","epoch_count = range(1, len(hist.history[list(hist.history.keys())[0]]) + 1)\n","sns.lineplot(x=epoch_count,  y=hist.history[list(hist.history.keys())[0]], label='train_loss')\n","sns.lineplot(x=epoch_count,  y=hist.history[list(hist.history.keys())[3]], label='valid_loss')\n","sns.lineplot(x=epoch_count,  y=hist.history[list(hist.history.keys())[1]], label='train_precision')\n","sns.lineplot(x=epoch_count,  y=hist.history[list(hist.history.keys())[4]], label='valid_precision')\n","sns.lineplot(x=epoch_count,  y=hist.history[list(hist.history.keys())[2]], label='train_recall')\n","sns.lineplot(x=epoch_count,  y=hist.history[list(hist.history.keys())[5]], label='valid_recall')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.evaluate(X_test, y_test)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
