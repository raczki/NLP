{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "- Tomar un ejemplo de los bots utilizados (uno de los dos) y construir el propio.\n",
    "- Sacar conclusiones de los resultados.\n",
    "\n",
    "__IMPORTANTE__: Recuerde para la entrega del ejercicio debe quedar registrado en el colab las preguntas y las respuestas del BOT para que podamos evaluar el desempeño final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\karen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\karen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\karen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\karen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\karen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re \n",
    "import nltk\n",
    "import urllib.request\n",
    "import unicodedata\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# BeautifulSoup y lxml para analizar y navegar por estructuras HTML y XML\n",
    "import lxml\n",
    "import bs4 as bs\n",
    "\n",
    "# Descargar recursos necesarios de NLTK\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_html = urllib.request.urlopen('https://es.wikipedia.org/wiki/Argentina')\n",
    "raw_html = raw_html.read()\n",
    "\n",
    "# Parsear artículo, 'lxml' es el parser a utilizar\n",
    "article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
    "\n",
    "# Eliminar todas las etiquetas <sup> y su contenido\n",
    "for sup_tag in article_html.find_all('sup'):\n",
    "    sup_tag.decompose()\n",
    "\n",
    "# Encontrar todos los párrafos y unirlos en un solo texto\n",
    "article_paragraphs = article_html.find_all('p')\n",
    "article_text = ''.join(para.text for para in article_paragraphs)\n",
    "\n",
    "article_text = ''\n",
    "\n",
    "for para in article_paragraphs:\n",
    "    article_text += para.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de caracteres en el articulo: 190770\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de caracteres en el articulo:\", len(article_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Quitar tildes y otros caracteres diacríticos\n",
    "    text = ''.join((c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn'))\n",
    "\n",
    "    # Remover caracteres especiales (manteniendo algunos signos de puntuación importantes)\n",
    "    pattern = r'[^a-zA-Z0-9.,!?/:;\\\"\\'\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "\n",
    "    # Sustituir más de un caracter de espacio, salto de línea o tabulación por un solo espacio\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Ahora aplicamos la función al texto del artículo\n",
    "preprocessed_article_text = preprocess_text(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de caracteres en el articulo: 188994\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de caracteres en el articulo:\", len(preprocessed_article_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividir el texto en oraciones y palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentencias: 924\n",
      "Palabras en las sentencias: 33952\n"
     ]
    }
   ],
   "source": [
    "# Dividir el texto en oraciones\n",
    "sentences = nltk.sent_tokenize(preprocessed_article_text, language='spanish')\n",
    "\n",
    "# Dividir cada sentencia en palabras\n",
    "words_in_sentences = nltk.word_tokenize(preprocessed_article_text, language='spanish')\n",
    "\n",
    "print(\"Sentencias:\", len(sentences))\n",
    "print(\"Palabras en las sentencias:\", len(words_in_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def perform_lemmatization(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def get_processed_text(document):\n",
    "    return nltk.word_tokenize(preprocess_text(document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se']\n"
     ]
    }
   ],
   "source": [
    "# Acceder a las stopwords en español\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "\n",
    "# Ejemplo de stopwords en español\n",
    "print(spanish_stopwords[:10])  # Imprime las primeras 10 stopwords para ejemplificar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se']\n"
     ]
    }
   ],
   "source": [
    "preprocessed_stopwords = []\n",
    "\n",
    "for w in spanish_stopwords:\n",
    "    preprocessed_word = preprocess_text(w)\n",
    "    preprocessed_stopwords.append(preprocessed_word)\n",
    "\n",
    "print(preprocessed_stopwords[:10])  # Imprime las primeras 10 stopwords preprocesadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(user_input, corpus):\n",
    "    response = ''\n",
    "\n",
    "    # Agrega la entrada del usuario al corpus\n",
    "    corpus.append(user_input)\n",
    "\n",
    "    # Inicializa el vectorizador TFIDF con tokens lematizados\n",
    "    word_vectorizer = TfidfVectorizer(tokenizer=get_processed_text, stop_words=preprocessed_stopwords)\n",
    "\n",
    "    # Vectoriza el corpus\n",
    "    all_word_vectors = word_vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Calcula similitud coseno con todos los documentos\n",
    "    similar_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors)\n",
    "    \n",
    "    # Encuentra el índice del documento más similar\n",
    "    similar_sentence_number = similar_vector_values.argsort()[0][-2]\n",
    "    matched_vector = similar_vector_values.flatten()\n",
    "    matched_vector.sort()\n",
    "    vector_matched = matched_vector[-2]\n",
    "\n",
    "    # Imprime el valor de similitud coseno\n",
    "    print('cosine similarity value: ', vector_matched)\n",
    "\n",
    "    # Determina la respuesta basada en la similitud\n",
    "    if vector_matched < 0.2:\n",
    "        response = \"Disculpame, no puedo responder a esa pregunta\"\n",
    "    else:\n",
    "        response = corpus[similar_sentence_number]\n",
    "    \n",
    "    # Elimina la entrada del usuario del corpus\n",
    "    corpus.remove(user_input)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity value:  0.4836918974330471\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'actualmente, el castellano es la lengua predominante, entendido y hablado como primera o segunda lengua por casi toda la poblacion de la argentina, que segun el censo de 2022 llega a 47,2 millones de habitantes.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo\n",
    "user_input = \"cual es la lengua predominante en argentina\"\n",
    "generate_response(user_input, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity value:  0.4629284275764496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'de acuerdo con el informe sobre desarrollo humano del programa de las naciones unidas para el desarrollo de 2018, la argentina tiene un indice de desarrollo humano idh de 0,825. a nivel mundial, se situa en el puesto 47 dentro de los 189 estados que participan en la clasificacion, encasillado como un pais de idh muy alto que junto con chile y uruguay son los unicos paises de america latina que se encuentran en este nivel de idh.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = \"cual es el IDH de argentina\"\n",
    "generate_response(user_input, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity value:  0.43562585331374387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'la economia argentina es la segunda mas desarrollada e importante en sudamerica.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = \"como es la economia argentina\"\n",
    "generate_response(user_input, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity value:  0.44543156095093006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'el escritor argentino ernesto sabato ha reflexionado sobre la naturaleza de la cultura argentina del siguiente modo: la cultura argentina tiene como origen la mezcla de otras que se encontraron durante los anos de las inmigraciones.'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = \"como es la cultura argentina\"\n",
    "generate_response(user_input, sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
