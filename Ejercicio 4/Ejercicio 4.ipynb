{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "\n",
    "Utilizar otro dataset y poner en práctica la predicción de próxima palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tfds-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'core' from partially initialized module 'tensorflow_datasets' (most likely due to a circular import) (c:\\Users\\karen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\karen\\OneDrive\\Escritorio\\Especializacion\\NLP\\Ejercicio 4\\Ejercicio 4.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/karen/OneDrive/Escritorio/Especializacion/NLP/Ejercicio%204/Ejercicio%204.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtfds\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\karen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\__init__.py:43\u001b[0m\n\u001b[0;32m     41\u001b[0m _TIMESTAMP_IMPORT_STARTS \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mabsl\u001b[39;00m \u001b[39mimport\u001b[39;00m logging\n\u001b[1;32m---> 43\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogging\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_tfds_logging\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogging\u001b[39;00m \u001b[39mimport\u001b[39;00m call_metadata \u001b[39mas\u001b[39;00m _call_metadata\n\u001b[0;32m     46\u001b[0m _metadata \u001b[39m=\u001b[39m _call_metadata\u001b[39m.\u001b[39mCallMetadata()\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'core' from partially initialized module 'tensorflow_datasets' (most likely due to a circular import) (c:\\Users\\karen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_datasets\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import io\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tfds.load('wikipedia/20200301.en', split='train')\n",
    "\n",
    "for example in dataset.take(10):  # Toma solo los primeros 10 ejemplos\n",
    "    text = example['text']\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datos\n",
    "Utilizaremos como dataset canciones de bandas de habla inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar la carpeta de dataset\n",
    "import platform\n",
    "if os.access('./songs_dataset', os.F_OK) is False:\n",
    "    if os.access('songs_dataset.zip', os.F_OK) is False:\n",
    "        if platform.system() == 'Windows':\n",
    "            !curl https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/datasets/songs_dataset.zip -o songs_dataset.zip\n",
    "        else:\n",
    "            !wget songs_dataset.zip https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/raw/main/datasets/songs_dataset.zip\n",
    "    !unzip -q songs_dataset.zip\n",
    "else:\n",
    "    print(\"El dataset ya se encuentra descargado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"./songs_dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armar el dataset utilizando salto de línea para separar las oraciones/docs\n",
    "# text_files = os.listdir(\"./songs_dataset/\")\n",
    "text_files = ['al-green.txt']\n",
    "df = pd.DataFrame()\n",
    "for text in text_files:\n",
    "    path = \"./songs_dataset/\" + text\n",
    "    df_i = pd.read_csv(path, sep='/n', header=None, engine='python')\n",
    "    df = pd.concat([df,df_i], ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cantidad de documentos:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - Preprocesamiento completo\n",
    "Debemos realizar los mismos pasos que en el ejemplo anterior, pero antes de eso debemos transformar ese dataset de filas de oraciones en un texto completo continuo para poder extraer el vocabulario.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer # equivalente a ltokenizer de nltk\n",
    "from keras.preprocessing.text import text_to_word_sequence # equivalente a word_tokenize de nltk\n",
    "from keras.utils import pad_sequences # se utilizará para padding\n",
    "\n",
    "# largo de la secuencia, incluye seq input + word output\n",
    "train_len = 6 # Cambie el train_len de 4 a 6 para aumentar la cantidad de palabra para el entrenamiento\n",
    "\n",
    "tok = Tokenizer()\n",
    "\n",
    "# El tokenizer \"aprende\" las palabras que se usaran\n",
    "# Se construye (fit) una vez por proyecto, se aplica N veces (tal cual un encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vistazo a las primeras filas\n",
    "df.loc[:15,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenamos todos los rows en un solo valor\n",
    "corpus = df.apply(lambda row: ' '.join(row.values.astype(str)), axis=0)[0]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar el corpus a tokens\n",
    "tokens=text_to_word_sequence(corpus)\n",
    "# Vistazo general de los primeros tokens\n",
    "tokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cantidad de tokens en el corpus:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código para hacer el desfasaje de las palabras\n",
    "# según el train_len\n",
    "text_sequences = []\n",
    "for i in range(train_len, len(tokens)):\n",
    "  seq = tokens[i-train_len:i]\n",
    "  text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demos un vistazo a nuestros vectores para entrenar el modelo\n",
    "text_sequences[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proceso de tokenización\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(text_sequences)\n",
    "\n",
    "# Convertimos las palabras a números\n",
    "# entran palabras -> salen números\n",
    "sequences = tok.texts_to_sequences(text_sequences)\n",
    "\n",
    "# Damos un vistazo\n",
    "sequences[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cantidad de rows del dataset:\", len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - Input y target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_sequences = np.array(sequences)\n",
    "x_data = arr_sequences[:,:-1]\n",
    "y_data_int = arr_sequences[:,-1] # aún falta el oneHotEncoder\n",
    "\n",
    "print(x_data.shape)\n",
    "print(y_data_int.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palabras del vocabulario\n",
    "tok.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cantidad de palabras en el vocabulario\n",
    "vocab_size = len(tok.word_counts)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En el caso anterior explota porque y_data_int comienza en \"1\" en vez de \"0\"\n",
    "# valor minimo:\n",
    "min(y_data_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_int_offset = y_data_int - 1\n",
    "y_data = to_categorical(y_data_int_offset, num_classes=vocab_size)\n",
    "y_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# largo de la secuencia de entrada\n",
    "input_seq_len = x_data.shape[1]\n",
    "input_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Largo del vector de salida --> vocab_size\n",
    "output_size = vocab_size\n",
    "output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Embedding:\n",
    "# input_seq_len = 5 --> ingreso 5 palabras\n",
    "# input_dim = vocab_size --> 1204 palabras distintas\n",
    "# output_dim = 13 --> crear embeddings de tamaño 5 (tamaño variable y ajustable) - (Surd[1204,4]= 5.9)\n",
    "model.add(Embedding(input_dim=vocab_size+1, output_dim=5, input_length=input_seq_len))\n",
    "\n",
    "model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(GRU(128))) # La última capa LSTM no lleva return_sequences\n",
    "model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Predicción de clasificación con softmax\n",
    "# La salida vuelve al espacio de 30459 palabras posibles\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "# Clasificación multiple categórica --> loss = categorical_crossentropy\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(x_data, y_data, epochs=70, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Entrenamiento\n",
    "epoch_count = range(1, len(hist.history['accuracy']) + 1)\n",
    "sns.lineplot(x=epoch_count,  y=hist.history['accuracy'], label='train')\n",
    "sns.lineplot(x=epoch_count,  y=hist.history['val_accuracy'], label='valid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generación de secuencias nuevas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(model, tokenizer, seed_text, max_length, n_words):\n",
    "    \"\"\"\n",
    "        Exec model sequence prediction\n",
    "\n",
    "        Args:\n",
    "            model (keras): modelo entrenado\n",
    "            tokenizer (keras tokenizer): tonenizer utilizado en el preprocesamiento\n",
    "            seed_text (string): texto de entrada (input_seq)\n",
    "            max_length (int): máxima longitud de la sequencia de entrada\n",
    "            n_words (int): números de palabras a agregar a la sequencia de entrada\n",
    "        returns:\n",
    "            output_text (string): sentencia con las \"n_words\" agregadas\n",
    "    \"\"\"\n",
    "    output_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "\t\t# Encodeamos\n",
    "        encoded = tokenizer.texts_to_sequences([output_text])[0]\n",
    "\t\t# Si tienen distinto largo\n",
    "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "\n",
    "\t\t# Predicción softmax\n",
    "        y_hat = model.predict(encoded).argmax(axis=-1)\n",
    "\t\t# Vamos concatenando las predicciones\n",
    "        out_word = ''\n",
    "\n",
    "        # Debemos buscar en el vocabulario la palabra\n",
    "        # que corresopnde al indice (y_hat) predicho por le modelo\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == y_hat:\n",
    "                out_word = word\n",
    "                break\n",
    "\n",
    "\t\t# Agrego las palabras a la frase predicha\n",
    "        output_text += ' ' + out_word\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text='I\\'m so in love with'\n",
    "\n",
    "generate_seq(model, tok, input_text, max_length=5, n_words=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beam search y muestreo aleatorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcionalidades para hacer encoding y decoding\n",
    "\n",
    "def encode(text,max_length=5):\n",
    "\n",
    "    encoded = tok.texts_to_sequences([text])[0]\n",
    "    encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "\n",
    "    return encoded\n",
    "\n",
    "def decode(seq):\n",
    "    return tok.sequences_to_texts([seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "# función que selecciona candidatos para el beam search\n",
    "def select_candidates(pred,num_beams,vocab_size,history_probs,history_tokens,temp=1):\n",
    "\n",
    "  # colectar todas las probabilidades para la siguiente búsqueda\n",
    "  pred_large = []\n",
    "\n",
    "  for idx,pp in enumerate(pred):\n",
    "    pred_large.extend(np.log(pp+1E-10)+history_probs[idx])\n",
    "\n",
    "  pred_large = np.array(pred_large)\n",
    "\n",
    "  # criterio de selección\n",
    "  # idx_select = np.argsort(pred_large)[::-1][:num_beams] # beam search determinista\n",
    "  idx_select = np.random.choice(np.arange(pred_large.shape[0]), num_beams, p=softmax(pred_large/temp)) # beam search con muestreo\n",
    "\n",
    "  # traducir a índices de token en el vocabulario\n",
    "  new_history_tokens = np.concatenate((np.array(history_tokens)[idx_select//vocab_size],\n",
    "                        np.array([idx_select%vocab_size]).T),\n",
    "                      axis=1)\n",
    "\n",
    "  # devolver el producto de las probabilidades (log) y la secuencia de tokens seleccionados\n",
    "  return pred_large[idx_select.astype(int)], new_history_tokens.astype(int)\n",
    "\n",
    "\n",
    "def beam_search(model,num_beams,num_words,input):\n",
    "\n",
    "    # first iteration\n",
    "\n",
    "    # encode\n",
    "    encoded = encode(input)\n",
    "\n",
    "    # first prediction\n",
    "    y_hat = np.squeeze(model.predict(encoded))\n",
    "\n",
    "    # get vocabulary size\n",
    "    vocab_size = y_hat.shape[0]\n",
    "\n",
    "    # initialize history\n",
    "    history_probs = [0]*num_beams\n",
    "    history_tokens = [encoded[0]]*num_beams\n",
    "\n",
    "    # select num_beams candidates\n",
    "    history_probs, history_tokens = select_candidates([y_hat],\n",
    "                                        num_beams,\n",
    "                                        vocab_size,\n",
    "                                        history_probs,\n",
    "                                        history_tokens)\n",
    "\n",
    "    # beam search loop\n",
    "    for i in range(num_words-1):\n",
    "\n",
    "      preds = []\n",
    "\n",
    "      for hist in history_tokens:\n",
    "\n",
    "        # actualizar secuencia de tokens\n",
    "        input_update = np.array([hist[i+1:]]).copy()\n",
    "\n",
    "        # predicción\n",
    "        y_hat = np.squeeze(model.predict(input_update))\n",
    "\n",
    "        preds.append(y_hat)\n",
    "\n",
    "      history_probs, history_tokens = select_candidates(preds,\n",
    "                                                        num_beams,\n",
    "                                                        vocab_size,\n",
    "                                                        history_probs,\n",
    "                                                        history_tokens)\n",
    "\n",
    "    return history_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicción con beam search\n",
    "salidas = beam_search(model,num_beams=10,num_words=6,input=\"Whether times are good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veamos las salidas\n",
    "decode(salidas[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo entrenado tuvo un muy mail desempeño en el entrenamiento además de overfitting. Cuestiones que podrían mejorarse:\n",
    "\n",
    "Agregar más capas o neuronaes\n",
    "Incrementar la cantidad de épocas\n",
    "Agregar BRNN\n",
    "Es importante destacar que en este ejemplo estamos entrenando nuestro propios Embeddings, y para ello se requiere mucha data. En los ejemplos que realizaremos de aquí en más utilizaremos más datos, embeddings pre-enternados o modelos pre-entrenados.\n",
    "\n",
    "Cambios que realice para mejorar el desempeño:\n",
    "\n",
    "Cambio en el dataset: Intente usar todas las canciones del dataset pero no pude correrlo en colab por el uso de la memoria, tambien intente usar los libros de \"Lord of the Ring\" pero tammbien el consumo de memoria ram supero el maximo en el mometo del entrenamiento. Encontre que con el artista 'al-green.txt' que el resultado de acuracy en la validacion fue un poco mejor pero todavia es baja y hay overfitting.\n",
    "Aumentar el train_len de 4 a 6\n",
    "Agregar bidireccionalidad\n",
    "Utilizar capaz GRU en lugar de LSTM\n",
    "Aumentar la cantidad de neuronas de 64 a 128\n",
    "Luego de entrenar el modelo agregando mas neuronas pude lograr una mejora en accuracy. Tambien probe agregando capaz pero no mejoro.\n",
    "\n",
    "A Difirencia del ejemplo visto en clase el acurracy en este caso fue de 20% peor todavia hay overfitting. Seria interesante ver que pasa con la totalidad de las camciones pero no lo puede entrenar.\n",
    "\n",
    "Tambien intente usar mas cantidad de artistas pero el accuracy nunca supero el 10%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
